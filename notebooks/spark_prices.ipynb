{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83c99b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f96319ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as T\n",
    "from pyspark.sql import Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "65a2f73d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "319e15ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "HOME = os.path.expanduser(\"~\")\n",
    "DATA_DIR = os.path.join(HOME,r'Data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5d616b7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/yevgeniy/Data/quant/prices'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prices_dir = os.path.join(DATA_DIR,r'quant/prices')\n",
    "prices_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "26924d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark=SparkSession.builder.master('spark://pop-os.localdomain:7077').appName('DataFrame-Kafka').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ee2df384",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spark.app.startTime', '1635364539742'),\n",
       " ('spark.executor.id', 'driver'),\n",
       " ('spark.driver.port', '50000'),\n",
       " ('spark.app.name', 'PySparkShell'),\n",
       " ('spark.sql.catalogImplementation', 'hive'),\n",
       " ('spark.rdd.compress', 'True'),\n",
       " ('spark.app.id', 'local-1635364540537'),\n",
       " ('spark.serializer.objectStreamReset', '100'),\n",
       " ('spark.sql.warehouse.dir',\n",
       "  'file:/Users/yevgeniy/Development/projects/data-engineering/data-engineering/notebooks/spark-warehouse'),\n",
       " ('spark.master', 'local[*]'),\n",
       " ('spark.submit.pyFiles', ''),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.ui.showConsoleProgress', 'true'),\n",
       " ('spark.driver.host', '192.168.0.25')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sparkContext.getConf().getAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9b9a042a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf_prices = spark.read.csv(os.path.join(prices_dir,'prices_5yrs.csv'), header=True,inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7497f18d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- date: string (nullable = true)\n",
      " |-- ticker: string (nullable = true)\n",
      " |-- open: double (nullable = true)\n",
      " |-- high: double (nullable = true)\n",
      " |-- low: double (nullable = true)\n",
      " |-- close: double (nullable = true)\n",
      " |-- volume: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf_prices.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "170f2d8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+------------------+------------------+------------------+------------------+---------+\n",
      "|      date|ticker|              open|              high|               low|             close|   volume|\n",
      "+----------+------+------------------+------------------+------------------+------------------+---------+\n",
      "|2016-04-11|  AAPL|25.338489526693984|25.719833996666296| 25.30593586028477|25.350114822387695|117630000|\n",
      "|2016-04-12|  AAPL|25.424524132308605|25.694256545777197| 25.26640733356858| 25.68030548095703|108929200|\n",
      "|2016-04-13|  AAPL|25.764012071449244| 26.12210236496072|25.764012071449244|26.052345275878906|133029200|\n",
      "|2016-04-14|  AAPL| 25.95468547387539|26.133730628820437|25.887252376182882| 26.06629753112793|101895600|\n",
      "|2016-04-15|  AAPL| 26.06862375416007|26.112804488531896|25.515209673368787| 25.54311180114746|187756000|\n",
      "+----------+------+------------------+------------------+------------------+------------------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf_prices.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6cc3d132",
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf_tickers = spark.read.csv(os.path.join(prices_dir,'yahoo_tickers.csv'), header=True,inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f3563394",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Ticker: string (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Exchange: string (nullable = true)\n",
      " |-- Category Name: string (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf_tickers.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "67c1939f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------------------------------------+--------+---------------------------+-------+\n",
      "|Ticker|Name                                   |Exchange|Category Name              |Country|\n",
      "+------+---------------------------------------+--------+---------------------------+-------+\n",
      "|OEDV  |Osage Exploration and Development, Inc.|PNK     |null                       |USA    |\n",
      "|AAPL  |Apple Inc.                             |NMS     |Electronic Equipment       |USA    |\n",
      "|BAC   |Bank of America Corporation            |NYQ     |Money Center Banks         |USA    |\n",
      "|AMZN  |Amazon.com, Inc.                       |NMS     |Catalog & Mail Order Houses|USA    |\n",
      "|T     |AT&T Inc.                              |NYQ     |Telecom Services - Domestic|USA    |\n",
      "+------+---------------------------------------+--------+---------------------------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf_tickers.show(5,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6105f7da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------+\n",
      "|FullName                                   |\n",
      "+-------------------------------------------+\n",
      "|OEDVOsage Exploration and Development, Inc.|\n",
      "|AAPLApple Inc.                             |\n",
      "|BACBank of America Corporation             |\n",
      "|AMZNAmazon.com, Inc.                       |\n",
      "|TAT&T Inc.                                 |\n",
      "+-------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf_tickers.withColumn(\"FullName\",(T.concat(T.col(\"Ticker\"),T.col(\"Name\"))))\\\n",
    "    .select(T.col(\"FullName\")).show(5,truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97bc90b1",
   "metadata": {},
   "source": [
    "### Joining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e16ef26a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5f82e07d",
   "metadata": {},
   "source": [
    "### Creating\n",
    "\n",
    "    * From Row object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "eaae49d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----+\n",
      "|      Authors|State|\n",
      "+-------------+-----+\n",
      "|Matei Zaharia|   CA|\n",
      "|  Reynold Xin|   CA|\n",
      "+-------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rows = [Row(\"Matei Zaharia\", \"CA\"), Row(\"Reynold Xin\", \"CA\")]\n",
    "authors_df = spark.createDataFrame(rows, [\"Authors\", \"State\"])\n",
    "authors_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d50380e",
   "metadata": {},
   "source": [
    "### Select"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "68b72b4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[date: string, ticker: string, open: double, high: double, low: double, close: double, volume: int]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sdf_prices.sort(T.col('date').desc())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ada396a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|ticker|\n",
      "+------+\n",
      "|  AAPL|\n",
      "+------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf_prices.select('ticker').show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e6d71808",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+\n",
      "|(ticker = AAPL)|\n",
      "+---------------+\n",
      "|           true|\n",
      "+---------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf_prices.select(sdf_prices['ticker'] == 'AAPL').show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "93b8e551",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+\n",
      "|      date|              open|\n",
      "+----------+------------------+\n",
      "|2016-04-11|25.338489526693984|\n",
      "+----------+------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf_prices.filter('open > 25').select(['date','open']).show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e659485",
   "metadata": {},
   "source": [
    "#### Iteration\n",
    "When you want to iterate through a DataFrame, you could use iterrows in pandas. In Spark, you create an array of rows using collect().\n",
    "\n",
    "The following code will use a filter method to get all people over open > 25 and print the array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "fb36fc1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_25_prices = sdf_prices.filter('open > 25').select(['date','open']).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "d56bd18a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(date='2016-04-11', open=25.338489526693984)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_25_prices[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac90a0e4",
   "metadata": {},
   "source": [
    "To get a single row, you can just pass the index. You can convert the row into different formats, and in this example, I converted it into a dictionary. As a dictionary, you can select any value by specifying the key. The code is shown as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "8649d2b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'date': '2016-04-11', 'open': 25.338489526693984}\n"
     ]
    }
   ],
   "source": [
    "for r in top_25_prices:\n",
    "    print(r.asDict())\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00174ad8",
   "metadata": {},
   "source": [
    "### Spark SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5188ee32",
   "metadata": {},
   "source": [
    "If you are more comfortable with SQL, you can filter a DataFrame using spark.sql. To use SQL, you must first create a view, then you can query it with SQL, as shown in the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "3d7db496",
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf_prices.createOrReplaceTempView('top_prices')\n",
    "top_25_prices_view = spark.sql('select * from top_prices where open > 25')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "d57d92a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+------------------+------------------+-----------------+------------------+---------+\n",
      "|      date|ticker|              open|              high|              low|             close|   volume|\n",
      "+----------+------+------------------+------------------+-----------------+------------------+---------+\n",
      "|2016-04-11|  AAPL|25.338489526693984|25.719833996666296|25.30593586028477|25.350114822387695|117630000|\n",
      "+----------+------+------------------+------------------+-----------------+------------------+---------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "top_25_prices_view.show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d583329",
   "metadata": {},
   "source": [
    "#### Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "462f1ea9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|summary|              open|\n",
      "+-------+------------------+\n",
      "|  count|             12590|\n",
      "|   mean|409.02570375165925|\n",
      "| stddev| 599.4995554732652|\n",
      "|    min| 21.05486413111501|\n",
      "|    max|            3547.0|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf_prices.describe('open').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "0111ca33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n",
      "|ticker|count|\n",
      "+------+-----+\n",
      "|  AAPL| 1259|\n",
      "|   JPM| 1259|\n",
      "|  TSLA| 1259|\n",
      "|  GOOG| 1259|\n",
      "|    FB| 1259|\n",
      "|  NFLX| 1259|\n",
      "|   WMT| 1259|\n",
      "|  AMZN| 1259|\n",
      "|  MSFT| 1259|\n",
      "|  ADBE| 1259|\n",
      "+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf_prices.groupBy('ticker').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "c5ff91e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+------------------+\n",
      "|count(ticker)|         avg(open)|\n",
      "+-------------+------------------+\n",
      "|        12590|409.02570375165925|\n",
      "+-------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Aggregate by various operations with dict\n",
    "sdf_prices.agg({'ticker' : 'count', 'open' : 'mean'}).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e37144",
   "metadata": {},
   "source": [
    "For both groupBy and agg, you can use mean, max, min, sum, and other methods\n",
    "that you can read about in the documentation. There is a large number of other functions you can use that require you to import the pyspark.sql.functions module.\n",
    "The following code imports it as f and demonstrates some useful functions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a50c667",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "277ef6238d877280f83ee7921511e90af0bd3018c18040f83d05ab519afa6ea7"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
