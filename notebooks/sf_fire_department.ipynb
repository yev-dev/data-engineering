{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c9ce87f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8334ad93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import pyspark as ps\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9467a08d",
   "metadata": {},
   "outputs": [],
   "source": [
    "HOME = os.path.expanduser(\"~\")\n",
    "DATA_DIR = os.path.join(HOME,r'Data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c60d3778",
   "metadata": {},
   "outputs": [],
   "source": [
    "databricks_dir = os.path.join(DATA_DIR,r'databricks')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0391786d",
   "metadata": {},
   "source": [
    "### Reading DataFrame\n",
    "\n",
    "Also we can infer a schema but a better option would be to define types programatically.\n",
    "\n",
    "The spark.read.csv() function reads in the CSV file and returns a DataFrame of rows and named columns with the types dictated in the schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bf03ab1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Programmatic way to define a schema \n",
    "fire_schema = StructType([StructField('CallNumber', IntegerType(), True),\n",
    "                StructField('UnitID', StringType(), True),\n",
    "                StructField('IncidentNumber', IntegerType(), True),\n",
    "                StructField('CallType', StringType(), True),                  \n",
    "                StructField('CallDate', StringType(), True),      \n",
    "                StructField('WatchDate', StringType(), True),\n",
    "                StructField('CallFinalDisposition', StringType(), True),\n",
    "                StructField('AvailableDtTm', StringType(), True),\n",
    "                StructField('Address', StringType(), True),       \n",
    "                StructField('City', StringType(), True),       \n",
    "                StructField('Zipcode', IntegerType(), True),       \n",
    "                StructField('Battalion', StringType(), True),                 \n",
    "                StructField('StationArea', StringType(), True),       \n",
    "                StructField('Box', StringType(), True),       \n",
    "                StructField('OriginalPriority', StringType(), True),       \n",
    "                StructField('Priority', StringType(), True),       \n",
    "                StructField('FinalPriority', IntegerType(), True),       \n",
    "                StructField('ALSUnit', BooleanType(), True),       \n",
    "                StructField('CallTypeGroup', StringType(), True),\n",
    "                StructField('NumAlarms', IntegerType(), True),\n",
    "                StructField('UnitType', StringType(), True),\n",
    "                StructField('UnitSequenceInCallDispatch', IntegerType(), True),\n",
    "                StructField('FirePreventionDistrict', StringType(), True),\n",
    "                StructField('SupervisorDistrict', StringType(), True),\n",
    "                StructField('Neighborhood', StringType(), True),\n",
    "                StructField('Location', StringType(), True),\n",
    "                StructField('RowID', StringType(), True),\n",
    "                StructField('Delay', FloatType(), True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8213c111",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the DataFrameReader interface to read a CSV file\n",
    "sf_fire_file = os.path.join(databricks_dir, r\"sf-fire-calls.csv\")\n",
    "fire_df = spark.read.csv(sf_fire_file, header=True, schema=fire_schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35622ba5",
   "metadata": {},
   "source": [
    "To write the DataFrame into an external data source in your format of choice, you can use the DataFrameWriter interface. Like DataFrameReader, it supports multiple data sources. Parquet, a popular columnar format, is the default format; it uses snappy compression to compress the data. If the DataFrame is written as Parquet, the schema is preserved as part of the Parquet metadata. In this case, subsequent reads back into a DataFrame do not require you to manually supply a schema."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b6b8d0",
   "metadata": {},
   "source": [
    "### Saving a DataFrame as a Parquet file or SQL table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b6f6abfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parquet_path = os.path.join(databricks_dir, r'f-fire-calls.parquet')\n",
    "# fire_df.write.format(\"parquet\").save(parquet_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9067beb0",
   "metadata": {},
   "source": [
    "### Transformations and actions\n",
    "\n",
    "Are they of the correct types? Do any of them need to be converted to different types? Do they have null values?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfdf7dac",
   "metadata": {},
   "source": [
    "#### Projections and filters\n",
    "\n",
    "A projection in relational parlance is a way to return only the rows matching a certain relational condition by using filters. In Spark, projections are done with the select() method, while filters can be expressed using the filter() or where() method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "89d4152a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------------------+--------------+\n",
      "|IncidentNumber|AvailableDtTm         |CallType      |\n",
      "+--------------+----------------------+--------------+\n",
      "|2003235       |01/11/2002 01:51:44 AM|Structure Fire|\n",
      "|2003250       |01/11/2002 04:16:46 AM|Vehicle Fire  |\n",
      "|2003259       |01/11/2002 06:01:58 AM|Alarms        |\n",
      "|2003279       |01/11/2002 08:03:26 AM|Structure Fire|\n",
      "|2003301       |01/11/2002 09:46:44 AM|Alarms        |\n",
      "+--------------+----------------------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "few_fire_df = (fire_df\n",
    "  .select(\"IncidentNumber\", \"AvailableDtTm\", \"CallType\") \n",
    "  .filter(col(\"CallType\") != \"Medical Incident\"))\n",
    "few_fire_df.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ebaf14b",
   "metadata": {},
   "source": [
    "What if we want to know how many distinct CallTypes were recorded as the causes of the fire calls? These simple and expressive queries do the job:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7cf0c046",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 2:=====================================>                 (136 + 4) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|DistinctCallTypes|\n",
      "+-----------------+\n",
      "|               30|\n",
      "+-----------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 2:=================================================>     (179 + 4) / 200]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "(fire_df\n",
    "  .select(\"CallType\")\n",
    "  .where(col(\"CallType\").isNotNull())\n",
    "  .agg(countDistinct(\"CallType\").alias(\"DistinctCallTypes\"))\n",
    "  .show())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28e03056",
   "metadata": {},
   "source": [
    "We can list the distinct call types in the data set using these queries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dbc81627",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------+\n",
      "|CallType                           |\n",
      "+-----------------------------------+\n",
      "|Elevator / Escalator Rescue        |\n",
      "|Marine Fire                        |\n",
      "|Aircraft Emergency                 |\n",
      "|Confined Space / Structure Collapse|\n",
      "|Administrative                     |\n",
      "|Alarms                             |\n",
      "|Odor (Strange / Unknown)           |\n",
      "|Citizen Assist / Service Call      |\n",
      "|HazMat                             |\n",
      "|Watercraft in Distress             |\n",
      "+-----------------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(fire_df\n",
    "  .select(\"CallType\")\n",
    "  .where(col(\"CallType\").isNotNull())\n",
    "  .distinct()\n",
    "  .show(10, False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f44ab471",
   "metadata": {},
   "source": [
    "#### Renaming, adding, and dropping columns\n",
    "\n",
    "By specifying the desired column names in the schema with StructField, as we did, we effectively changed all names in the resulting DataFrame.\n",
    "\n",
    "Alternatively, you could selectively rename columns with the **withColumnRenamed()** method. For instance, let’s change the name of our Delay column to ResponseDelayedinMins and take a look at the response times that were longer than five minutes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "44ee1a2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+\n",
      "|ResponseDelayedinMins|\n",
      "+---------------------+\n",
      "|5.35                 |\n",
      "|6.25                 |\n",
      "|5.2                  |\n",
      "|5.6                  |\n",
      "|7.25                 |\n",
      "+---------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_fire_df = fire_df.withColumnRenamed(\"Delay\", \"ResponseDelayedinMins\")\n",
    "(new_fire_df\n",
    "  .select(\"ResponseDelayedinMins\")\n",
    "  .where(col(\"ResponseDelayedinMins\") > 5)\n",
    "  .show(5, False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da9285fe",
   "metadata": {},
   "source": [
    "To convert into a more usable format. **spark.sql.functions** has a set of to/from date/timestamp functions such as to_timestamp() and to_date()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "780a915a",
   "metadata": {},
   "source": [
    "    * Convert the existing column’s data type from string to a Spark-supported timestamp.\n",
    "    * Use the new format specified in the format string \"MM/dd/yyyy\" or \"MM/dd/yyyy hh:mm:ss a\" where appropriate.\n",
    "    * After converting to the new data type, drop() the old column and append the new one specified in the first argument to the withColumn() method.\n",
    "    * Assign the new modified DataFrame to fire_ts_df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "55fe9976",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'new_fire_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/x6/4b3wnhrd6wvdc7n118kl2qkh0000gn/T/ipykernel_2350/2605775801.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# In Python\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m fire_ts_df = (new_fire_df\n\u001b[0m\u001b[1;32m      3\u001b[0m   \u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"IncidentDate\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mto_timestamp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"CallDate\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"MM/dd/yyyy\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"CallDate\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"OnWatchDate\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mto_timestamp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"WatchDate\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"MM/dd/yyyy\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'new_fire_df' is not defined"
     ]
    }
   ],
   "source": [
    "# In Python\n",
    "fire_ts_df = (new_fire_df\n",
    "  .withColumn(\"IncidentDate\", to_timestamp(col(\"CallDate\"), \"MM/dd/yyyy\"))\n",
    "  .drop(\"CallDate\") \n",
    "  .withColumn(\"OnWatchDate\", to_timestamp(col(\"WatchDate\"), \"MM/dd/yyyy\"))\n",
    "  .drop(\"WatchDate\") \n",
    "  .withColumn(\"AvailableDtTS\", to_timestamp(col(\"AvailableDtTm\"), \n",
    "  \"MM/dd/yyyy hh:mm:ss a\"))\n",
    "  .drop(\"AvailableDtTm\"))\n",
    "\n",
    "# Select the converted columns\n",
    "(fire_ts_df\n",
    "  .select(\"IncidentDate\", \"OnWatchDate\", \"AvailableDtTS\")\n",
    "  .show(5, False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45419242",
   "metadata": {},
   "source": [
    "Now that we have modified the dates, we can query using functions from spark.sql.functions like dayofmonth(), dayofyear(), and dayofweek() to explore our data further. \n",
    "\n",
    "We could find out how many calls were logged in the last seven days, or we could see how many years’ worth of Fire Department calls are included in the data set with this query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "00a5da01",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 19:==============>                                           (1 + 3) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|year(IncidentDate)|\n",
      "+------------------+\n",
      "|              2000|\n",
      "|              2001|\n",
      "|              2002|\n",
      "|              2003|\n",
      "|              2004|\n",
      "|              2005|\n",
      "|              2006|\n",
      "|              2007|\n",
      "|              2008|\n",
      "|              2009|\n",
      "|              2010|\n",
      "|              2011|\n",
      "|              2012|\n",
      "|              2013|\n",
      "|              2014|\n",
      "|              2015|\n",
      "|              2016|\n",
      "|              2017|\n",
      "|              2018|\n",
      "+------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "(fire_ts_df\n",
    "  .select(year('IncidentDate'))\n",
    "  .distinct()\n",
    "  .orderBy(year('IncidentDate'))\n",
    "  .show())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba7dd9e4",
   "metadata": {},
   "source": [
    "#### Aggregations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76166bbb",
   "metadata": {},
   "source": [
    "Let’s take our first question: what were the most common types of fire calls?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5f35369b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------+------+\n",
      "|CallType                       |count |\n",
      "+-------------------------------+------+\n",
      "|Medical Incident               |113794|\n",
      "|Structure Fire                 |23319 |\n",
      "|Alarms                         |19406 |\n",
      "|Traffic Collision              |7013  |\n",
      "|Citizen Assist / Service Call  |2524  |\n",
      "|Other                          |2166  |\n",
      "|Outside Fire                   |2094  |\n",
      "|Vehicle Fire                   |854   |\n",
      "|Gas Leak (Natural and LP Gases)|764   |\n",
      "|Water Rescue                   |755   |\n",
      "+-------------------------------+------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/10/27 23:36:14 WARN HeartbeatReceiver: Removing executor driver with no recent heartbeats: 142029 ms exceeds timeout 120000 ms\n",
      "21/10/27 23:36:14 WARN SparkContext: Killing executors is not supported by current scheduler.\n"
     ]
    }
   ],
   "source": [
    "(fire_ts_df\n",
    "  .select(\"CallType\")\n",
    "  .where(col(\"CallType\").isNotNull())\n",
    "  .groupBy(\"CallType\")\n",
    "  .count()\n",
    "  .orderBy(\"count\", ascending=False)\n",
    "  .show(n=10, truncate=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c65fd4b",
   "metadata": {},
   "source": [
    "The DataFrame API also offers the collect() method, but for extremely large DataFrames this is resource-heavy (expensive) and dangerous, as it can cause out-of-memory (OOM) exceptions. Unlike count(), which returns a single number to the driver, collect() returns a collection of all the Row objects in the entire DataFrame or Dataset. If you want to take a peek at some Row records you’re better off with take(n), which will return only the first n Row objects of the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "81f4382c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------------------+--------------------------+--------------------------+\n",
      "|sum(NumAlarms)|avg(ResponseDelayedinMins)|min(ResponseDelayedinMins)|max(ResponseDelayedinMins)|\n",
      "+--------------+--------------------------+--------------------------+--------------------------+\n",
      "|        176170|         3.892364154521585|               0.016666668|                   1844.55|\n",
      "+--------------+--------------------------+--------------------------+--------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(fire_ts_df\n",
    "  .select(F.sum(\"NumAlarms\"), F.avg(\"ResponseDelayedinMins\"),\n",
    "    F.min(\"ResponseDelayedinMins\"), F.max(\"ResponseDelayedinMins\"))\n",
    "  .show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6f9f07ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- CallNumber: integer (nullable = true)\n",
      " |-- UnitID: string (nullable = true)\n",
      " |-- IncidentNumber: integer (nullable = true)\n",
      " |-- CallType: string (nullable = true)\n",
      " |-- CallFinalDisposition: string (nullable = true)\n",
      " |-- Address: string (nullable = true)\n",
      " |-- City: string (nullable = true)\n",
      " |-- Zipcode: integer (nullable = true)\n",
      " |-- Battalion: string (nullable = true)\n",
      " |-- StationArea: string (nullable = true)\n",
      " |-- Box: string (nullable = true)\n",
      " |-- OriginalPriority: string (nullable = true)\n",
      " |-- Priority: string (nullable = true)\n",
      " |-- FinalPriority: integer (nullable = true)\n",
      " |-- ALSUnit: boolean (nullable = true)\n",
      " |-- CallTypeGroup: string (nullable = true)\n",
      " |-- NumAlarms: integer (nullable = true)\n",
      " |-- UnitType: string (nullable = true)\n",
      " |-- UnitSequenceInCallDispatch: integer (nullable = true)\n",
      " |-- FirePreventionDistrict: string (nullable = true)\n",
      " |-- SupervisorDistrict: string (nullable = true)\n",
      " |-- Neighborhood: string (nullable = true)\n",
      " |-- Location: string (nullable = true)\n",
      " |-- RowID: string (nullable = true)\n",
      " |-- ResponseDelayedinMins: float (nullable = true)\n",
      " |-- IncidentDate: timestamp (nullable = true)\n",
      " |-- OnWatchDate: timestamp (nullable = true)\n",
      " |-- AvailableDtTS: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fire_ts_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48fda5b4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
